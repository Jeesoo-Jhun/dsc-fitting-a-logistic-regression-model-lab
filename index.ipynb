{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fitting a Logistic Regression Model - Lab\n","\n","## Introduction\n","\n","In the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you'll be investigating fitting logistic regressions with `statsmodels`. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the [Titanic](https://www.kaggle.com/c/titanic/data) shipwreck or not (yes, it's a bit morbid).\n","\n","\n","## Objectives\n","\n","In this lab you will: \n","\n","* Implement logistic regression with `statsmodels` \n","* Interpret the statistical results associated with model parameters"]},{"cell_type":"markdown","metadata":{},"source":["## Import the data\n","\n","Import the data stored in the file `'titanic.csv'` and print the first five rows of the DataFrame to check its contents. "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  \n"]}],"source":["import pandas as pd\n","\n","# Import the data\n","df = pd.read_csv('titanic.csv')\n","print(df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["## Define independent and target variables\n","\n","Your target variable is in the column `'Survived'`. A `0` indicates that the passenger didn't survive the shipwreck. Print the total number of people who didn't survive the shipwreck. How many people survived?"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Survived\n","0    549\n","1    342\n","Name: count, dtype: int64\n"]}],"source":["# Total number of people who survived/didn't survive\n","survival_counts = df['Survived'].value_counts()\n","print(survival_counts)\n"]},{"cell_type":"markdown","metadata":{},"source":["Only consider the columns specified in `relevant_columns` when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type `float`: "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["(891, 8)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Create dummy variables\n","relevant_columns = ['Pclass', 'Age', 'SibSp', 'Fare', 'Sex', 'Embarked', 'Survived']\n","df_relevant = df[relevant_columns]\n","dummy_dataframe = pd.get_dummies(df_relevant, drop_first=True).astype(float)\n","\n","dummy_dataframe.shape"]},{"cell_type":"markdown","metadata":{},"source":["Did you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values. \n","\n","> NOTE: You can use the [`.dropna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) method to do this. "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["(714, 8)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Drop missing rows\n","dummy_dataframe = dummy_dataframe.dropna()\n","dummy_dataframe.shape"]},{"cell_type":"markdown","metadata":{},"source":["Finally, assign the independent variables to `X` and the target variable to `y`: "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Split the data into X and y\n","y = dummy_dataframe['Survived']\n","X = dummy_dataframe.drop(columns=['Survived'])"]},{"cell_type":"markdown","metadata":{},"source":["## Fit the model\n","\n","Now with everything in place, you can build a logistic regression model using `statsmodels` (make sure you create an intercept term as we showed in the previous lesson).  \n","\n","> Warning: Did you receive an error of the form \"LinAlgError: Singular matrix\"? This means that `statsmodels` was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.443267\n","         Iterations 6\n","                           Logit Regression Results                           \n","==============================================================================\n","Dep. Variable:               Survived   No. Observations:                  714\n","Model:                          Logit   Df Residuals:                      706\n","Method:                           MLE   Df Model:                            7\n","Date:                Mon, 04 Nov 2024   Pseudo R-squ.:                  0.3437\n","Time:                        10:54:00   Log-Likelihood:                -316.49\n","converged:                       True   LL-Null:                       -482.26\n","Covariance Type:            nonrobust   LLR p-value:                 1.103e-67\n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          5.6503      0.633      8.921      0.000       4.409       6.892\n","Pclass        -1.2118      0.163     -7.433      0.000      -1.531      -0.892\n","Age           -0.0431      0.008     -5.250      0.000      -0.059      -0.027\n","SibSp         -0.3806      0.125     -3.048      0.002      -0.625      -0.136\n","Fare           0.0012      0.002      0.474      0.636      -0.004       0.006\n","Sex_male      -2.6236      0.217    -12.081      0.000      -3.049      -2.198\n","Embarked_Q    -0.8260      0.598     -1.381      0.167      -1.999       0.347\n","Embarked_S    -0.4130      0.269     -1.533      0.125      -0.941       0.115\n","==============================================================================\n"]}],"source":["import statsmodels.api as sm\n","\n","# Add an intercept term to the model\n","X = sm.add_constant(X)\n","\n","# Fit the logistic regression model\n","logit_model = sm.Logit(y, X)\n","result = logit_model.fit()\n","\n","# Print the summary of the model\n","print(result.summary())\n"]},{"cell_type":"markdown","metadata":{},"source":["## Analyze results\n","\n","Generate the summary table for your model. Then, comment on the p-values associated with the various features you chose."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Summary table\n","# Above there"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your comments here\n","\n","# The p-values indicate the statistical significance of each feature in the model:\n","# - Pclass, Age, SibSp, and Sex_male have very low p-values (< 0.05), indicating that they are statistically significant predictors of survival.\n","# - Fare, Embarked_Q, and Embarked_S have high p-values (> 0.05), suggesting that they are not statistically significant predictors of survival in this model.\n","# - The intercept (const) is also statistically significant with a very low p-value.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Level up (Optional)\n","\n","Create a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.445882\n","         Iterations 6\n","                           Logit Regression Results                           \n","==============================================================================\n","Dep. Variable:               Survived   No. Observations:                  714\n","Model:                          Logit   Df Residuals:                      709\n","Method:                           MLE   Df Model:                            4\n","Date:                Mon, 04 Nov 2024   Pseudo R-squ.:                  0.3399\n","Time:                        10:56:02   Log-Likelihood:                -318.36\n","converged:                       True   LL-Null:                       -482.26\n","Covariance Type:            nonrobust   LLR p-value:                 1.089e-69\n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          5.6008      0.543     10.306      0.000       4.536       6.666\n","Pclass        -1.3174      0.141     -9.350      0.000      -1.594      -1.041\n","Age           -0.0444      0.008     -5.442      0.000      -0.060      -0.028\n","SibSp         -0.3761      0.121     -3.106      0.002      -0.613      -0.139\n","Sex_male      -2.6235      0.215    -12.229      0.000      -3.044      -2.203\n","==============================================================================\n"]}],"source":["# Select only the influential features\n","X_influential = X[['const', 'Pclass', 'Age', 'SibSp', 'Sex_male']]\n","\n","# Fit the logistic regression model with the influential features\n","logit_model_influential = sm.Logit(y, X_influential)\n","result_influential = logit_model_influential.fit()\n","\n","# Print the summary of the new model\n","print(result_influential.summary())\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","used /explain (rerun without)\n","The output provided is the result of a logistic regression analysis. Let's break down the key components of the results:\n","\n","Key Components\n","Dependent Variable (Dep. Variable): Survived\n","\n","This indicates that the model is predicting the likelihood of survival.\n","Number of Observations (No. Observations): 714\n","\n","The dataset contains 714 observations.\n","Model: Logit\n","\n","This specifies that a logistic regression model was used.\n","Method: MLE\n","\n","Maximum Likelihood Estimation was used to estimate the model parameters.\n","Pseudo R-squared (Pseudo R-squ.): 0.3399\n","\n","This is a measure of how well the model explains the variability of the outcome. A value of 0.3399 indicates that approximately 33.99% of the variability in the dependent variable is explained by the model.\n","Log-Likelihood: -318.36\n","\n","This is a measure of the model fit. Higher values (closer to zero) indicate a better fit.\n","Converged: True\n","\n","This indicates that the optimization algorithm successfully converged.\n","LL-Null: -482.26\n","\n","The log-likelihood of the null model (a model with no predictors).\n","LLR p-value: 1.089e-69\n","\n","The p-value for the likelihood ratio test comparing the fitted model to the null model. A very small p-value indicates that the model provides a significantly better fit than the null model.\n","Coefficients and Statistics\n","const: 5.6008\n","\n","The intercept of the model. A positive value indicates a higher baseline log-odds of survival.\n","Pclass: -1.3174\n","\n","The coefficient for passenger class. A negative value indicates that higher classes (lower numerical values) are associated with higher odds of survival.\n","Age: -0.0444\n","\n","The coefficient for age. A negative value indicates that older age is associated with lower odds of survival.\n","SibSp: -0.3761\n","\n","The coefficient for the number of siblings/spouses aboard. A negative value indicates that having more siblings/spouses aboard is associated with lower odds of survival.\n","Sex_male: -2.6235\n","\n","The coefficient for being male. A negative value indicates that being male is associated with lower odds of survival.\n","Statistical Significance\n","P>|z|: The p-values for each coefficient.\n","All p-values are very small (0.000 or close to it), indicating that all predictors are statistically significant.\n","Confidence Intervals\n","[0.025, 0.975]: The 95% confidence intervals for each coefficient.\n","These intervals provide a range within which the true coefficient values are likely to fall.\n","Interpretation\n","The model suggests that being in a higher passenger class, being older, having more siblings/spouses aboard, and being male are all associated with lower odds of survival.\n","The intercept and coefficients are statistically significant, indicating that these predictors have a meaningful impact on the likelihood of survival."]},{"cell_type":"markdown","metadata":{},"source":["## Summary \n","\n","Well done! In this lab, you practiced using `statsmodels` to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":2}
